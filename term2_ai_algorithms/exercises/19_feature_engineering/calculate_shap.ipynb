{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Shapley values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapley values as used in coalition game theory were introduced by William Shapley in 1953.  \n",
    "[Scott Lundberg](http://scottlundberg.com/) applied Shapley values for calculating feature importance in [2017](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf).  \n",
    "\n",
    "If you want to read the paper, I recommend reading:  \n",
    "Abstract, 1 Introduction, 2 Additive Feature Attribution Methods, (skip 2.1, 2.2, 2.3), and 2.4 Classic Shapley Value Estimation.\n",
    "\n",
    "Lundberg calls this feature importance method \"SHAP\", which stands for SHapley Additive exPlanations.\n",
    "\n",
    "Here’s the formula for calculating Shapley values:\n",
    "\n",
    "$ \\phi_{i} = \\sum_{S \\subseteq M \\setminus i} \\frac{|S|! (|M| - |S| -1 )!}{|M|!} [f(S \\cup i) - f(S)]$\n",
    "\n",
    "A key part of this is the difference between the model’s prediction with the feature $i$, and the model’s prediction without feature $i$.  \n",
    "$S$ refers to a subset of features that doesn’t include the feature for which we're calculating $\\phi_i$.  \n",
    "$S \\cup i$ is the subset that includes features in $S$ plus feature $i$.  \n",
    "$S \\subseteq M \\setminus i$ in the $\\Sigma$ symbol is saying, all sets $S$ that are subsets of the full set of features $M$, excluding feature $i$.  \n",
    "\n",
    "##### Options for your learning journey\n",
    "* If you’re okay with just using this formula, you can skip ahead to the coding section below.   \n",
    "* If you would like an explanation for what this formula is doing, please continue reading here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional (explanation of this formula)\n",
    "\n",
    "The part of the formula with the factorials calculates the number of ways to generate the collection of features, where order matters.\n",
    "\n",
    "$\\frac{|S|! (|M| - |S| -1 )!}{|M|!}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding features to a Coalition\n",
    "\n",
    "The following concepts come from coalition game theory, so when we say \"coalition\", think of it as a team, where members of the team are added, one after another, in a particular order.\n",
    "\n",
    "Let’s imagine that we’re creating a coalition of features, by adding one feature at a time to the coalition, and including all $|M|$ features.  Let’s say we have 3 features total.  Here are all the possible ways that we can create this “coalition” of features.\n",
    "\n",
    "<ol>\n",
    "    <li>$x_0,x_1,x_2$</li>\n",
    "    <li>$x_0,x_2,x_1$</li>\n",
    "    <li>$x_1,x_0,x_2$</li>\n",
    "    <li>$x_1,x_2,x_0$</li>\n",
    "    <li>$x_2,x_0,x_1$</li>\n",
    "    <li>$x_2,x_1,x_0$</li>\n",
    "</ol>\n",
    "\n",
    "Notice that for $|M| = 3$ features, there are $3! = 3 \\times 2 \\times 1 = 6$ possible ways to create the coalition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### marginal contribution of a feature\n",
    "\n",
    "For each of the 6 ways to create a coalition, let's see how to calculate the marginal contribution of feature $x_2$.\n",
    "\n",
    "<ol>\n",
    "<li>Model’s prediction when it includes features 0,1,2, minus the model’s prediction when it includes only features 0 and 1.  \n",
    "\n",
    "$x_0,x_1,x_2$: $f(x_0,x_1,x_2) - f(x_0,x_1)$  \n",
    "\n",
    "\n",
    "<li>Model’s prediction when it includes features 0 and 2, minus the prediction when using only feature 0.  Notice that feature 1 is added after feature 2, so it’s not included in the model.  \n",
    "    \n",
    "$x_0,x_2,x_1$: $f(x_0,x_2) - f(x_0)$</li>\n",
    "\n",
    "\n",
    "<li>Model's prediction including all three features, minus when the model is only given features 1 and 0.  \n",
    "\n",
    "$x_1,x_0,x_2$: $f(x_1,x_0,x_2) - f(x_1,x_0)$</li>\n",
    "\n",
    "\n",
    "<li>Model's prediction when given features 1 and 2, minus when the model is only given feature 1.  \n",
    "    \n",
    "$x_1,x_2,x_0$: $f(x_1,x_2) - f(x_1)$</li>\n",
    "\n",
    "\n",
    "<li>Model’s prediction if it only uses feature 2, minus the model’s prediction if it has no features.  When there are no features, the model’s prediction would be the average of the labels in the training data.  \n",
    "    \n",
    "$x_2,x_0,x_1$: $f(x_2) - f( )$\n",
    "</li>\n",
    "\n",
    "\n",
    "<li>Model's prediction (same as the previous one)  \n",
    "    \n",
    "$x_2,x_1,x_0$: $f(x_2) - f( )$\n",
    "</li>\n",
    "\n",
    "Notice that some of these marginal contribution calculations look the same.  For example the first and third sequences, $f(x_0,x_1,x_2) - f(x_0,x_1)$ would get the same result as $f(x_1,x_0,x_2) - f(x_1,x_0)$.  Same with the fifth and sixth.  So we can use factorials to help us calculate the number of permutations that result in the same marginal contribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### break into 2 parts\n",
    "\n",
    "To get to the formula that we saw above, we can break up the sequence into two sections: the sequence of features before adding feature $i$; and the sequence of features that are added after feature $i$.\n",
    "\n",
    "For the set of features that are added before feature $i$, we’ll call this set $S$.  For the set of features that are added after feature $i$ is added, we’ll call this $Q$.\n",
    "\n",
    "So, given the six sequences, and that feature $i$ is $x_2$ in this example, here’s what set $S$ and $Q$ are for each sequence:\n",
    "\n",
    "<ol>\n",
    "    <li>$x_0,x_1,x_2$: $S$ = {0,1}, $Q$ = {}</li>\n",
    "    <li>$x_0,x_2,x_1$: $S$ = {0},   $Q$ = {1}  </li>\n",
    "    <li>$x_1,x_0,x_2$: $S$ = {1,0}, $Q$ = {}  </li>\n",
    "    <li>$x_1,x_2,x_0$: $S$ = {1}, $Q$ = {0}  </li>\n",
    "    <li>$x_2,x_0,x_1$: $S$ = {}, $Q$ = {0,1}  </li>\n",
    "    <li>$x_2,x_1,x_0$: $S$ = {}, $Q$ = {1,0}  </li>\n",
    "</ol>\n",
    "So for the first and third sequences, these have the same set S = {0,1} and same set $Q$ = {}.  \n",
    "Another way to calculate that there are two of these sequences is to take $|S|! \\times |Q|! = 2! \\times 0! = 2$.\n",
    "\n",
    "Similarly, the fifth and sixth sequences have the same set S = {} and Q = {0,1}.  \n",
    "Another way to calculate that there are two of these sequences is to take $|S|! \\times |Q|! = 0! \\times 2! = 2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And now, the original formula\n",
    "\n",
    "To use the notation of the original formula, note that $|Q| = |M| - |S| - 1$.\n",
    "\n",
    "Recall that to calculate that there are 6 total sequences, we can use $|M|! = 3! = 3 \\times 2 \\times 1 = 6$.  \n",
    "We’ll divide $|S|! \\times (|M| - |S| - 1)!$ by $|M|!$ to get the proportion assigned to each marginal contribution.   \n",
    "This is the weight that will be applied to each marginal contribution, and the weights sum to 1.\n",
    "\n",
    "So that’s how we get the formula:  \n",
    "\n",
    "$\\frac{|S|! (|M| - |S| -1 )!}{|M|!} [f(S \\cup i) - f(S)]$  \n",
    "\n",
    "for each set $S \\subseteq M \\setminus i$\n",
    "\n",
    "We can sum up the weighted marginal contributions for all sets $S$, and this represents the importance of feature $i$.\n",
    "\n",
    "You’ll get to practice this in code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.14.5\n",
      "  Downloading https://files.pythonhosted.org/packages/68/1e/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.2MB 38kB/s  eta 0:00:01  9% |███                             | 1.1MB 27.0MB/s eta 0:00:01    30% |█████████▊                      | 3.7MB 25.5MB/s eta 0:00:01    40% |█████████████                   | 4.9MB 27.1MB/s eta 0:00:01    70% |██████████████████████▊         | 8.6MB 26.3MB/s eta 0:00:01    90% |█████████████████████████████   | 11.0MB 25.6MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Found existing installation: numpy 1.12.1\n",
      "    Uninstalling numpy-1.12.1:\n",
      "      Successfully uninstalled numpy-1.12.1\n",
      "Successfully installed numpy-1.14.5\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: scikit-learn==0.19.1 in /opt/conda/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting graphviz==0.9\n",
      "  Downloading https://files.pythonhosted.org/packages/47/87/313cd4ea4f75472826acb74c57f94fc83e04ba93e4ccf35656f6b7f502e2/graphviz-0.9-py2.py3-none-any.whl\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.9\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting shap==0.25.2\n",
      "  Downloading https://files.pythonhosted.org/packages/0b/85/196abf4c178446e077237e4c649e93637393dac24ab7f760143c4ef1548e/shap-0.25.2.tar.gz (197kB)\n",
      "\u001b[K    100% |████████████████████████████████| 204kB 2.2MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.6/site-packages (from shap==0.25.2)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->shap==0.25.2)\n",
      "Requirement already satisfied: python-dateutil>=2.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib->shap==0.25.2)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from matplotlib->shap==0.25.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib->shap==0.25.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->shap==0.25.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.15 in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.6/site-packages (from ipython->shap==0.25.2)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from prompt-toolkit<2.0.0,>=1.0.15->ipython->shap==0.25.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython->shap==0.25.2)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.6/site-packages (from traitlets>=4.2->ipython->shap==0.25.2)\n",
      "Building wheels for collected packages: shap\n",
      "  Running setup.py bdist_wheel for shap ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3f/cb/58/2482f534eb1c1f9ac93653887aee5b6ea83718ef041c1310bd\n",
      "Successfully built shap\n",
      "Installing collected packages: shap\n",
      "Successfully installed shap-0.25.2\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy==1.14.5\n",
    "!{sys.executable} -m pip install scikit-learn==0.19.1\n",
    "!{sys.executable} -m pip install graphviz==0.9\n",
    "!{sys.executable} -m pip install shap==0.25.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import shap\n",
    "import numpy as np\n",
    "import graphviz\n",
    "from math import factorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate input data and fit a tree model\n",
    "We'll create data where features 0 and 1 form the \"AND\" operator, and feature 2 does not contribute to the prediction (because it's always zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"310pt\" height=\"261pt\"\n",
       " viewBox=\"0.00 0.00 310.00 261.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 257)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-257 306,-257 306,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.250980\" stroke=\"black\" d=\"M166.5,-253C166.5,-253 71.5,-253 71.5,-253 65.5,-253 59.5,-247 59.5,-241 59.5,-241 59.5,-201 59.5,-201 59.5,-195 65.5,-189 71.5,-189 71.5,-189 166.5,-189 166.5,-189 172.5,-189 178.5,-195 178.5,-201 178.5,-201 178.5,-241 178.5,-241 178.5,-247 172.5,-253 166.5,-253\"/>\n",
       "<text text-anchor=\"start\" x=\"87.5\" y=\"-238.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">X</text>\n",
       "<text text-anchor=\"start\" x=\"98.5\" y=\"-238.8\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">1</text>\n",
       "<text text-anchor=\"start\" x=\"106.5\" y=\"-238.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"74\" y=\"-224.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 0.188</text>\n",
       "<text text-anchor=\"start\" x=\"67.5\" y=\"-210.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100</text>\n",
       "<text text-anchor=\"start\" x=\"73.5\" y=\"-196.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 0.25</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M98,-147.5C98,-147.5 12,-147.5 12,-147.5 6,-147.5 0,-141.5 0,-135.5 0,-135.5 0,-106.5 0,-106.5 0,-100.5 6,-94.5 12,-94.5 12,-94.5 98,-94.5 98,-94.5 104,-94.5 110,-100.5 110,-106.5 110,-106.5 110,-135.5 110,-135.5 110,-141.5 104,-147.5 98,-147.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-132.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-117.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-102.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 0.0</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M98.75,-188.992C91.9408,-178.566 84.3073,-166.877 77.3462,-156.218\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"80.0883,-154.015 71.6899,-147.556 74.2274,-157.843 80.0883,-154.015\"/>\n",
       "<text text-anchor=\"middle\" x=\"66.5549\" y=\"-168.323\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.501961\" stroke=\"black\" d=\"M226,-153C226,-153 140,-153 140,-153 134,-153 128,-147 128,-141 128,-141 128,-101 128,-101 128,-95 134,-89 140,-89 140,-89 226,-89 226,-89 232,-89 238,-95 238,-101 238,-101 238,-141 238,-141 238,-147 232,-153 226,-153\"/>\n",
       "<text text-anchor=\"start\" x=\"151.5\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">X</text>\n",
       "<text text-anchor=\"start\" x=\"162.5\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">0</text>\n",
       "<text text-anchor=\"start\" x=\"170.5\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"142.5\" y=\"-124.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 0.25</text>\n",
       "<text text-anchor=\"start\" x=\"136\" y=\"-110.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n",
       "<text text-anchor=\"start\" x=\"142\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 0.5</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M139.25,-188.992C144.911,-180.323 151.143,-170.782 157.083,-161.685\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"160.125,-163.429 162.662,-153.142 154.264,-159.601 160.125,-163.429\"/>\n",
       "<text text-anchor=\"middle\" x=\"167.797\" y=\"-173.909\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M162,-53C162,-53 76,-53 76,-53 70,-53 64,-47 64,-41 64,-41 64,-12 64,-12 64,-6 70,-0 76,-0 76,-0 162,-0 162,-0 168,-0 174,-6 174,-12 174,-12 174,-41 174,-41 174,-47 168,-53 162,-53\"/>\n",
       "<text text-anchor=\"start\" x=\"83\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"72\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 25</text>\n",
       "<text text-anchor=\"start\" x=\"78\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 0.0</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M161.511,-88.9415C155.402,-80.1118 148.717,-70.451 142.496,-61.4586\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"145.274,-59.3229 136.706,-53.0908 139.518,-63.3058 145.274,-59.3229\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M290,-53C290,-53 204,-53 204,-53 198,-53 192,-47 192,-41 192,-41 192,-12 192,-12 192,-6 198,-0 204,-0 204,-0 290,-0 290,-0 296,-0 302,-6 302,-12 302,-12 302,-41 302,-41 302,-47 296,-53 290,-53\"/>\n",
       "<text text-anchor=\"start\" x=\"211\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"200\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 25</text>\n",
       "<text text-anchor=\"start\" x=\"206\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 1.0</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M204.489,-88.9415C210.598,-80.1118 217.283,-70.451 223.504,-61.4586\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"226.482,-63.3058 229.294,-53.0908 220.726,-59.3229 226.482,-63.3058\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7ff9764019e8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AND case (features 0 and 1)\n",
    "N = 100\n",
    "M = 3\n",
    "X = np.zeros((N,M))\n",
    "X.shape\n",
    "y = np.zeros(N)\n",
    "X[:1 * N//4, 1] = 1\n",
    "X[:N//2, 0] = 1\n",
    "X[N//2:3 * N//4, 1] = 1\n",
    "y[:1 * N//4] = 1\n",
    "\n",
    "# fit model\n",
    "model = sklearn.tree.DecisionTreeRegressor(random_state=0)\n",
    "model.fit(X, y)\n",
    "\n",
    "# draw model\n",
    "dot_data = sklearn.tree.export_graphviz(model, out_file=None, filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Shap values\n",
    "\n",
    "We'll try to calculate the local feature importance of feature 0.  \n",
    "\n",
    "We have 3 features, $x_0, x_1, x_2$.  For feature $x_0$, determine what the model predicts with or without $x_0$.  \n",
    "\n",
    "Subsets S that exclude feature $x_0$ are:  \n",
    "{}  \n",
    "{$x_1$}  \n",
    "{$x_2$}  \n",
    "{$x_1,x_2$}  \n",
    "\n",
    "We want to see what the model predicts with feature $x_0$ compared to the model without feature $x_0$:  \n",
    "$f(x_0) - f( )$  \n",
    "$f(x_0,x_1) - f(x_1)$   \n",
    "$f(x_0,x_2) - f(x_2)$  \n",
    "$f(x_0,x_1,x_2) - f(x_1,x_2)$  \n",
    "\n",
    "## Sample data point\n",
    "We'll calculate the local feature importance of a sample data point, where  \n",
    "feature $x_0 = 1$   \n",
    "feature $x_1 = 1$  \n",
    "feature $x_2 = 1$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample values to calculate local feature importance on: [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "sample_values = np.array([1,1,1])\n",
    "print(f\"sample values to calculate local feature importance on: {sample_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper function\n",
    "\n",
    "To make things easier, we'll use a helper function that takes the entire feature set M, and also a list of the features (columns) that we want, and puts them together into a 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset(X, feature_l):\n",
    "    \"\"\"\n",
    "    Given a 2D array containing all feature columns,\n",
    "    and a list of integers representing which columns we want,\n",
    "    Return a 2D array with just the subset of features desired\n",
    "    \"\"\"\n",
    "    cols_l = []\n",
    "    for f in feature_l:\n",
    "        cols_l.append(X[:,f].reshape(-1,1))\n",
    "        \n",
    "    return np.concatenate(cols_l, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try it out\n",
    "tmp = get_subset(X,[0,2])\n",
    "tmp[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper function to calculate permutation weight\n",
    "\n",
    "This helper function calculates  \n",
    "\n",
    "$\\frac{|S|! (|M| - |S| - 1)!}{|M|!}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial\n",
    "def calc_weight(size_S, num_features):\n",
    "    return factorial(size_S) * factorial(num_features - size_S - 1) / factorial(num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out when size of S is 2 and there are 3 features total.  \n",
    "The answer should be equal to $\\frac{2! \\times (3-2-1)!}{3!} = \\frac{2 \\times 1}{6} = \\frac{1}{3}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_weight(size_S=2,num_features=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## case A  \n",
    "\n",
    "Calculate the prediction of a model that uses features 0 and 1  \n",
    "Calculate the prediction of a model that uses feature 1  \n",
    "Calculate the difference (the marginal contribution of feature 0)\n",
    "\n",
    "$f(x_0,x_1) - f(x_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate $f(x_0,x_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S_union_i\n",
    "S_union_i = get_subset(X,[0,1])\n",
    "\n",
    "# fit model\n",
    "f_S_union_i = sklearn.tree.DecisionTreeRegressor()\n",
    "f_S_union_i.fit(S_union_i, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, for the sample input for which we'll calculate feature importance, we chose values of 1 for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 2D array, got 1D array instead:\n",
      "array=[1. 1.].\n",
      "Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n"
     ]
    }
   ],
   "source": [
    "# This will throw an error\n",
    "\n",
    "try:\n",
    "    f_S_union_i.predict(np.array([1,1]))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error message says:\n",
    "\n",
    ">Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n",
    "\n",
    "So we'll reshape the data so that it represents a sample (a row), which means it has 1 row and 1 or more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature 0 and feature 1 are both 1 in the sample input\n",
    "sample_input = np.array([1,1]).reshape(1,-1)\n",
    "sample_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction of the model when it has features 0 and 1 is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_S_union_i = f_S_union_i.predict(sample_input)\n",
    "pred_S_union_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When feature 0 and feature 1 are both 1, the prediction of the model is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate $f(x_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S\n",
    "S = get_subset(X,[1])\n",
    "f_S = sklearn.tree.DecisionTreeRegressor()\n",
    "f_S.fit(S, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample input for feature 1 is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = np.array([1]).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's prediction when it is only training on feature 1 is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_S = f_S.predict(sample_input)\n",
    "pred_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When feature 1 is 1, then the prediction of this model is 0.5.  If you look at the data in X, this makes sense, because when feature 1 is 1, half of the time, the label in y is 0, and half the time, the label in y is 1. So on average, the prediction is 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_A = pred_S_union_i - pred_S\n",
    "diff_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the weight\n",
    "Calculate the weight assigned to the marginal contribution.  In this case, if this marginal contribution occurs 1 out of the 6 possible permutations of the 3 features, then its weight is 1/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_S = S.shape[1] # should be 1\n",
    "weight_A = calc_weight(size_S, M)\n",
    "weight_A # should be 1/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Case B\n",
    "\n",
    "Calculate the prediction of a model that uses features 0 and 2  \n",
    "Calculate the prediction of a model that uses feature 2  \n",
    "Calculate the difference\n",
    "\n",
    "$f(x_0,x_2) - f(x_2)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate $f(x_0,x_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "S_union_i = get_subset(X,[0,2])\n",
    "f_S_union_i = sklearn.tree.DecisionTreeRegressor()\n",
    "f_S_union_i.fit(S_union_i, y)\n",
    "\n",
    "sample_input = np.array([1,1]).reshape(1,-1)\n",
    "pred_S_union_i = f_S_union_i.predict(sample_input)\n",
    "pred_S_union_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using features 0 and 2, and feature 2 doesn't help with predicting the output, then the model really just depends on feature 0.  When feature 0 is 1, half of the labels are 0, and half of the labels are 1.  So the average prediction is 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate $f(x_2)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "S = get_subset(X,[2])\n",
    "f_S = sklearn.tree.DecisionTreeRegressor()\n",
    "f_S.fit(S, y)\n",
    "\n",
    "sample_input = np.array([1]).reshape(1,-1)\n",
    "pred_S = f_S.predict(sample_input)\n",
    "pred_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since feature 2 doesn't help with predicting the labels in y, and feature 2 is 0 for all 100 training observations, then the prediction of the model is the average of all 100 training labels.  1/4 of the labels are 1, and the rest are 0.  So that prediction is 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the difference in predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "diff_B = pred_S_union_i - pred_S\n",
    "diff_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "size_S = S.shape[1] # is 1\n",
    "weight_B = calc_weight(size_S, M)\n",
    "weight_B # should be 1/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz: Case C\n",
    "\n",
    "Calculate the prediction of a model that uses features 0,1 and 2  \n",
    "Calculate the prediction of a model that uses feature 1 and 2  \n",
    "Calculate the difference\n",
    "\n",
    "$f(x_0,x_1,x_2) - f(x_1,x_2)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate $f(x_0,x_1,x_2) $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "S_union_i = get_subset(X,[0,1,2])\n",
    "f_S_union_i = sklearn.tree.DecisionTreeRegressor()\n",
    "f_S_union_i.fit(S_union_i, y)\n",
    "\n",
    "sample_input = np.array([1,1,1]).reshape(1,-1)\n",
    "pred_S_union_i = f_S_union_i.predict(sample_input)\n",
    "pred_S_union_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use all three features, the model is able to predict that if feature 0 and feature 1 are both 1, then the label is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate $f(x_1,x_2)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "S = get_subset(X,[1,2])\n",
    "f_S = sklearn.tree.DecisionTreeRegressor()\n",
    "f_S.fit(S, y)\n",
    "\n",
    "sample_input = np.array([1,1]).reshape(1,-1)\n",
    "pred_S = f_S.predict(sample_input)\n",
    "pred_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is trained on features 1 and 2, then its training data tells it that half of the time, when feature 1 is 1, the label is 0; and half the time, the label is 1.  So the average prediction of the model is 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate difference in predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "diff_C = pred_S_union_i - pred_S\n",
    "diff_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "size_S = S.shape[1]\n",
    "weight_C = calc_weight(size_S,M) # should be 2 / 6 = 1/3\n",
    "weight_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: case D: remember to include the empty set!\n",
    "\n",
    "The empty set is also a set.  We'll compare how the model does when it has no features, and see how that compares to when it gets feature 0 as input.\n",
    "\n",
    "Calculate the prediction of a model that uses features 0.  \n",
    "Calculate the prediction of a model that uses no features.  \n",
    "Calculate the difference\n",
    "\n",
    "$f(x_0) - f()$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate $f(x_0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "S_union_i = get_subset(X,[0])\n",
    "f_S_union_i = sklearn.tree.DecisionTreeRegressor()\n",
    "f_S_union_i.fit(S_union_i, y)\n",
    "\n",
    "sample_input = np.array([1]).reshape(1,-1)\n",
    "pred_S_union_i = f_S_union_i.predict(sample_input)\n",
    "pred_S_union_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just feature 0 as input, the model predicts 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate $f()$\n",
    "**hint**: you don't have to fit a model, since there are no features to input into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# with no input features, the model will predict the average of the labels, which is 0.25\n",
    "pred_S = np.mean(y)\n",
    "pred_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no input features, the model's best guess is the average of the labels, which is 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate difference in predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "diff_D = pred_S_union_i - pred_S\n",
    "diff_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate weight\n",
    "\n",
    "We expect this to be: 0! * (3-0-1)! / 3! = 2/6 = 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "size_S = 0\n",
    "weight_D = calc_weight(size_S,M) # weight is 1/3\n",
    "weight_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Shapley value\n",
    "For a single sample observation, where feature 0 is 1, feature 1 is 1, and feature 2 is 1, calculate the shapley value of feature 0 as the weighted sum of the differences in predictions.\n",
    "\n",
    "$\\phi_{i} = \\sum_{S \\subseteq N \\setminus i} weight_S \\times (f(S \\cup i) - f(S))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "shap_0 = weight_A * diff_A + weight_B * diff_B + weight_C * diff_C + weight_D * diff_D\n",
    "shap_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify with the shap library\n",
    "\n",
    "The [shap](https://github.com/slundberg/shap) library is written by Scott Lundberg, the creator of Shapley Additive Explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_values = np.array([1,1,1])\n",
    "shap_values = shap.TreeExplainer(model).shap_values(sample_values)\n",
    "\n",
    "print(f\"Shapley value for feature 0 that we calculated: {shap_0}\")\n",
    "print(f\"Shapley value for feature 0 is {shap_values[0]}\")\n",
    "print(f\"Shapley value for feature 1 is {shap_values[1]}\")\n",
    "print(f\"Shapley value for feature 2 is {shap_values[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Does this make sense?\n",
    "\n",
    "The shap libary outputs the shap values for features 0, 1 and 2.  We can see that the shapley value for feature 0 matches what we calculated.  The Shapley value for feature 1 is also given the same importance as feature 0.  \n",
    "* Given that the training data is simulating an AND operation, do you think these values make sense?  \n",
    "* Do you think feature 0 and 1 are equally important, or is one more important than the other?  \n",
    "* Does the importane of feature 2 make sense as well?\n",
    "* How does this compare to the feature importance that's built into sci-kit learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "This method is general enough that it works for any model, not just trees.  There is an optimized way to calculate this when the complex model being explained is a tree-based model.  We'll look at that next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "[Solution notebook](calculate_shap_solution.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
