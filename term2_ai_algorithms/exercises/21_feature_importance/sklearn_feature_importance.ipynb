{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance method in sci-kit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll get a sense of how feature importance is calculated in sci-kit learn, and also see where it gives results that we wouldn't expect.\n",
    "\n",
    "Sci-kit learn uses gini impurity to calculate a measure of impurity for each node.  Gini impurity, like entropy is a way to measure how \"disorganized\" the observations are before and after splitting them using a feature. So there is an impurity measure for each node.\n",
    "\n",
    "In the formula, freq_{i} is the frequency of label \"i\".  C is the number of unique labels at that node.\n",
    "\n",
    "$Impurity= \\sum_{i=1}^{C} - freq_{i} * (1- freq_{i})$\n",
    "\n",
    "The node importance in sci-kit learn is calculated as the difference between the gini impurity of the node and the gini impurity of its left and right children.  These gini impurities are weighted by the number of data points that reach each node.\n",
    "\n",
    "$NodeImportance = w_{i} Impurity_{i} - ( w_{left} Impurity_{left} + w_{right} Impurity_{right} )$\n",
    "\n",
    "The importance of a feature is the importance of the node that it was split on, divided by the sum of all node importances in the tree.  You’ll get to practice this in the coding exercise coming up next!\n",
    "\n",
    "For additional reading, please check out this blog post [The Mathematics of Decision Trees, Random Forest and Feature Importance in Scikit-learn and Spark](https://medium.com/@srnghn/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.14.5\n",
      "  Downloading https://files.pythonhosted.org/packages/68/1e/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.2MB 39kB/s  eta 0:00:01   34% |███████████▏                    | 4.2MB 35.7MB/s eta 0:00:01    60% |███████████████████▌            | 7.4MB 36.4MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Found existing installation: numpy 1.12.1\n",
      "    Uninstalling numpy-1.12.1:\n",
      "      Successfully uninstalled numpy-1.12.1\n",
      "Successfully installed numpy-1.14.5\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: scikit-learn==0.19.1 in /opt/conda/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting graphviz==0.9\n",
      "  Downloading https://files.pythonhosted.org/packages/47/87/313cd4ea4f75472826acb74c57f94fc83e04ba93e4ccf35656f6b7f502e2/graphviz-0.9-py2.py3-none-any.whl\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.9\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy==1.14.5\n",
    "!{sys.executable} -m pip install scikit-learn==0.19.1\n",
    "!{sys.executable} -m pip install graphviz==0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "\n",
    "We'll generate features and labels that form the \"AND\" operator.  So when feature 0 and feature 1 are both 1, then the label is 1, else the label is 0.  The third feature, feature 2, won't have an effect on the label output (it's always zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Features 0 and 1 form the AND operator\n",
    "Feature 2 is always zero.\n",
    "\"\"\"\n",
    "N = 100\n",
    "M = 3\n",
    "X = np.zeros((N,M))\n",
    "X.shape\n",
    "y = np.zeros(N)\n",
    "X[:1 * N//4, 1] = 1\n",
    "X[:N//2, 0] = 1\n",
    "X[N//2:3 * N//4, 1] = 1\n",
    "y[:1 * N//4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe the features\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe the labels\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tree.DecisionTreeClassifier(random_state=0)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the trained decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"345pt\" height=\"261pt\"\n",
       " viewBox=\"0.00 0.00 345.00 261.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 257)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-257 341,-257 341,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.666667\" stroke=\"black\" d=\"M186.5,-253C186.5,-253 79.5,-253 79.5,-253 73.5,-253 67.5,-247 67.5,-241 67.5,-241 67.5,-201 67.5,-201 67.5,-195 73.5,-189 79.5,-189 79.5,-189 186.5,-189 186.5,-189 192.5,-189 198.5,-195 198.5,-201 198.5,-201 198.5,-241 198.5,-241 198.5,-247 192.5,-253 186.5,-253\"/>\n",
       "<text text-anchor=\"start\" x=\"101.5\" y=\"-238.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">X</text>\n",
       "<text text-anchor=\"start\" x=\"112.5\" y=\"-238.8\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">1</text>\n",
       "<text text-anchor=\"start\" x=\"120.5\" y=\"-238.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"89.5\" y=\"-224.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.375</text>\n",
       "<text text-anchor=\"start\" x=\"81.5\" y=\"-210.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100</text>\n",
       "<text text-anchor=\"start\" x=\"75.5\" y=\"-196.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [75, 25]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M110,-147.5C110,-147.5 12,-147.5 12,-147.5 6,-147.5 -7.10543e-15,-141.5 -7.10543e-15,-135.5 -7.10543e-15,-135.5 -7.10543e-15,-106.5 -7.10543e-15,-106.5 -7.10543e-15,-100.5 6,-94.5 12,-94.5 12,-94.5 110,-94.5 110,-94.5 116,-94.5 122,-100.5 122,-106.5 122,-106.5 122,-135.5 122,-135.5 122,-141.5 116,-147.5 110,-147.5\"/>\n",
       "<text text-anchor=\"start\" x=\"26.5\" y=\"-132.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-117.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-102.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 0]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M110.219,-188.992C102.482,-178.461 93.7988,-166.643 85.9047,-155.898\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"88.5175,-153.543 79.7761,-147.556 82.8763,-157.688 88.5175,-153.543\"/>\n",
       "<text text-anchor=\"middle\" x=\"75.9966\" y=\"-168.569\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M259.5,-153C259.5,-153 152.5,-153 152.5,-153 146.5,-153 140.5,-147 140.5,-141 140.5,-141 140.5,-101 140.5,-101 140.5,-95 146.5,-89 152.5,-89 152.5,-89 259.5,-89 259.5,-89 265.5,-89 271.5,-95 271.5,-101 271.5,-101 271.5,-141 271.5,-141 271.5,-147 265.5,-153 259.5,-153\"/>\n",
       "<text text-anchor=\"start\" x=\"174.5\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">X</text>\n",
       "<text text-anchor=\"start\" x=\"185.5\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">0</text>\n",
       "<text text-anchor=\"start\" x=\"193.5\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"171.5\" y=\"-124.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"159\" y=\"-110.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n",
       "<text text-anchor=\"start\" x=\"148.5\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [25, 25]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M156.098,-188.992C162.622,-180.234 169.809,-170.585 176.648,-161.404\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"179.635,-163.253 182.802,-153.142 174.021,-159.071 179.635,-163.253\"/>\n",
       "<text text-anchor=\"middle\" x=\"186.419\" y=\"-174.179\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M185,-53C185,-53 87,-53 87,-53 81,-53 75,-47 75,-41 75,-41 75,-12 75,-12 75,-6 81,-0 87,-0 87,-0 185,-0 185,-0 191,-0 197,-6 197,-12 197,-12 197,-41 197,-41 197,-47 191,-53 185,-53\"/>\n",
       "<text text-anchor=\"start\" x=\"101.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"89\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 25</text>\n",
       "<text text-anchor=\"start\" x=\"83\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [25, 0]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M182.496,-88.9415C175.745,-80.0207 168.353,-70.2517 161.488,-61.1807\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"164.191,-58.9529 155.366,-53.0908 158.61,-63.177 164.191,-58.9529\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\n",
       "<path fill=\"#399de5\" stroke=\"black\" d=\"M325,-53C325,-53 227,-53 227,-53 221,-53 215,-47 215,-41 215,-41 215,-12 215,-12 215,-6 221,-0 227,-0 227,-0 325,-0 325,-0 331,-0 337,-6 337,-12 337,-12 337,-41 337,-41 337,-47 331,-53 325,-53\"/>\n",
       "<text text-anchor=\"start\" x=\"241.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"229\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 25</text>\n",
       "<text text-anchor=\"start\" x=\"223\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 25]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M229.504,-88.9415C236.255,-80.0207 243.647,-70.2517 250.512,-61.1807\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"253.39,-63.177 256.634,-53.0908 247.809,-58.9529 253.39,-63.177\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f24adb05048>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_data = sklearn.tree.export_graphviz(model, out_file=None, filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the tree\n",
    "\n",
    "The [source code for Tree](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx) has useful comments about attributes in the Tree class.  Search for the code that says `cdef class Tree:` for useful comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Tree object\n",
    "tree0 = model.tree_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree attributes are stored in lists\n",
    "\n",
    "The tree data are stored in lists.  Each node is also assigned an integer 0,1,2...  \n",
    "Each node's value for some attribute is stored at the index location that equals the node's assigned integer.\n",
    "\n",
    "For example, node 0 is the root node at the top of the tree.  There is a list called children_left.  Index location 0 contains the left child of node 0.\n",
    "\n",
    "\n",
    "#### left and right child nodes\n",
    "```\n",
    "children_left : array of int, shape [node_count]\n",
    "        children_left[i] holds the node id of the left child of node i.\n",
    "        For leaves, children_left[i] == TREE_LEAF. Otherwise,\n",
    "        children_left[i] > i. This child handles the case where\n",
    "        X[:, feature[i]] <= threshold[i].\n",
    "children_right : array of int, shape [node_count]\n",
    "        children_right[i] holds the node id of the right child of node i.\n",
    "        For leaves, children_right[i] == TREE_LEAF. Otherwise,\n",
    "        children_right[i] > i. This child handles the case where\n",
    "        X[:, feature[i]] > threshold[i].\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.children_left: [ 1 -1  3 -1 -1]\n",
      "tree0.children_right: [ 2 -1  4 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree0.children_left: {tree0.children_left}\")\n",
    "print(f\"tree0.children_right: {tree0.children_right}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this tree, the index positions 0,1,2,3,4 are the numbers for identifying each node in the tree.  Node 0 is the root node.  Node 1 and 2 are the left and right child of the root node.  So in the list children_left, at index 0, we see 1, and for children_right list, at index 0, we see 2.  \n",
    "\n",
    "-1 is used to denote that there is no child for that node.  Node 1 has no left or right child, so in the children_left list, at index 1, we see -1.  Similarly, in children_right, at index 1, the value is also -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### features used for splitting at each node\n",
    "```\n",
    "    feature : array of int, shape [node_count]\n",
    "        feature[i] holds the feature to split on, for the internal node i.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.feature: [ 1 -2  0 -2 -2]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree0.feature: {tree0.feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature 1 is used to split on node 0.  Feature 0 is used to split on node 2.  The -2 values indicate that these are leaf nodes (no features are used for splitting at those nodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of samples in each node\n",
    "\n",
    "```\n",
    "n_node_samples : array of int, shape [node_count]\n",
    "        n_node_samples[i] holds the number of training samples reaching node i.\n",
    "\n",
    "weighted_n_node_samples : array of int, shape [node_count]\n",
    "        weighted_n_node_samples[i] holds the weighted number of training samples\n",
    "        reaching node i.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.n_node_samples : [100  50  50  25  25]\n",
      "tree0.weighted_n_node_samples : [100.  50.  50.  25.  25.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree0.n_node_samples : {tree0.n_node_samples}\")\n",
    "print(f\"tree0.weighted_n_node_samples : {tree0.weighted_n_node_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted_n_node_samples is the same as n_node_samples for decision trees.  It's different for random forests where a sub-sample of data points is used in each tree.  We'll use weighted_n_node_samples in the code below, but either one works when we're calculating the proportion of samples in a left or right child node relative to their parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini impurity\n",
    "\n",
    "Gini impurity, like entropy is a way to measure how \"disorganized\" the observations are before and after splitting them using a feature. So there is an impurity value calculated for each node.\n",
    "\n",
    "In the formula, $freq_{i}$ is the frequency of label \"i\".  C is the number of unique labels at that node (C stands for \"Class\", as in \"classifier\".\n",
    "\n",
    "$ \\sum_{i}^{C} - freq_{i} * (1- freq_{i})$\n",
    "\n",
    "```\n",
    "impurity : array of double, shape [node_count]\n",
    "        impurity[i] holds the impurity (i.e., the value of the splitting\n",
    "        criterion) at node i.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the impurity if there is a single class (unique label type)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impurity of a homogenous sample with a single label, is: 0\n"
     ]
    }
   ],
   "source": [
    "freq0 = 1\n",
    "impurity = -1 * freq0 * (1 - freq0)\n",
    "print(f\"impurity of a homogenous sample with a single label, is: {impurity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the impurity if there are two classes (two distinct labels), and there are 90% of samples for one label, and 10% for the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impurity when 90% are of one label, and 10% are of the other: -0.18\n"
     ]
    }
   ],
   "source": [
    "freq1 = 0.9\n",
    "freq2 = 0.1\n",
    "impurity = -1 * freq1 * (1 -freq1) + -1 * freq2 * (1 - freq2)\n",
    "print(f\"impurity when 90% are of one label, and 10% are of the other: {impurity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "What is the impurity if there are two classes of label, and there are 50% of samples for one label, and 50% for the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impurity when 50% are of one label, and 50% are of the other: -0.5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "What is the impurity if there are two classes of label,\n",
    "and there are 50% of samples for one label, and 50% for the other?\n",
    "\"\"\"\n",
    "# TODO\n",
    "freq1 = 0.5\n",
    "freq2 = 0.5\n",
    "# TODO\n",
    "impurity = -1* 0.5 * 0.5 - 1*0.5*0.5\n",
    "print(f\"impurity when 50% are of one label, and 50% are of the other: {impurity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "Is the impurity larger or smaller (in magnitude) when the samples are dominated by a single class?  \n",
    "Is the impurity larger or smaller (in magnitude) when the frequency of each class is more evenly distributed among classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "smaller\n",
    "\n",
    "larger\n",
    "\n",
    "Gini impurity is anologuous to entropy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Importance\n",
    "\n",
    "The node importance in sklearn is calculated as the difference between the gini impurity of the node and the impurities of its child nodes.  These gini impurities are weighted by the number of data points that reach each node.\n",
    "\n",
    "$NodeImportance = w_{i} Impurity_{i} - ( w_{left} Impurity_{left} + w_{right} Impurity_{right} )$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of the node labels\n",
    "Node 0 is the root node, and its left and right children are 1 and 2.  \n",
    "Node 1 is a leaf node  \n",
    "Node 2 has two children, 3 and 4.  \n",
    "Node 3 is a leaf node  \n",
    "Node 4 is a leaf node  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.children_left: [ 1 -1  3 -1 -1]\n",
      "tree0.children_right: [ 2 -1  4 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# summary of child nodes\n",
    "print(f\"tree0.children_left: {tree0.children_left}\")\n",
    "print(f\"tree0.children_right: {tree0.children_right}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the node importance of the root node, node 0.  Its child nodes are 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance of node 0 is 12.5\n"
     ]
    }
   ],
   "source": [
    "ni0 = tree0.weighted_n_node_samples[0] * tree0.impurity[0] - \\\n",
    "        ( tree0.weighted_n_node_samples[1] * tree0.impurity[1] + \\\n",
    "          tree0.weighted_n_node_samples[2] * tree0.impurity[2] )\n",
    "print(f\"Importance of node 0 is {ni0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "Calculate the node importance of node 2.  Its left and right child nodes are 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance of node 2 is 25.0\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "ni2 = tree0.weighted_n_node_samples[2] * tree0.impurity[2] - \\\n",
    "        ( tree0.weighted_n_node_samples[3] * tree0.impurity[3] + \\\n",
    "          tree0.weighted_n_node_samples[4] * tree0.impurity[4] )\n",
    "print(f\"Importance of node 2 is {ni2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other nodes are leaf nodes, so there is no decrease in impurity that we can calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum the node importances\n",
    "Only nodes 0 and node 2 have node importances.  The others are leaf nodes, so we don't calculate node importances (there is no feature that is used for splitting at those leaf nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of node importances is 37.5\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "ni_total = ni0 + ni2\n",
    "print(f\"The sum of node importances is {ni_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of which feature is used to split at each node\n",
    "\n",
    "feature 0 was used to split on node 2  \n",
    "feature 1 was used to split on node 0  \n",
    "feature 2 was not used for splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree0.feature: [ 1 -2  0 -2 -2]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tree0.feature: {tree0.feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Calculate importance of the features\n",
    "\n",
    "The importance of a feature is the importance of the node that it was used for splitting, divided by the total node importances.  Calculate the importance of feature 0, 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importance of feature 0: 0.6666666666666666\n",
      "importance of feature 1: 0.3333333333333333\n",
      "importance of feature 2: 0\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "fi0 = ni2 / ni_total\n",
    "fi1 = ni0 / ni_total\n",
    "fi2 = 0\n",
    "print(f\"importance of feature 0: {fi0}\")\n",
    "print(f\"importance of feature 1: {fi1}\")\n",
    "print(f\"importance of feature 2: {fi2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double check with sklearn\n",
    "\n",
    "Check out how to use [feature importance](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn importance of feature 0: 0.6666666666666666\n",
      "sklearn importance of feature 1: 0.3333333333333333\n",
      "sklearn importance of feature 2: 0.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: get feature importances from sci-kit learn\n",
    "\n",
    "fi0_skl = model.feature_importances_[0]\n",
    "fi1_skl = model.feature_importances_[1]\n",
    "fi2_skl = model.feature_importances_[2]\n",
    "\n",
    "print(f\"sklearn importance of feature 0: {fi0_skl}\")\n",
    "print(f\"sklearn importance of feature 1: {fi1_skl}\")\n",
    "print(f\"sklearn importance of feature 2: {fi2_skl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notice anything odd?\n",
    "\n",
    "Notice that the data we generated simulates an AND operator.  If feature 0 and feature 1 are both 1, then the output is 1, otherwise 0.  So, from that perspective, do you think that features 0 and 1 are equally important?\n",
    "\n",
    "What do you notice about the feature importance calculated in sklearn?  Are the features considered equally important according to this calculation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "If someone tells you that you don't need to understand the algorithm, just how to install the package and call the function, do you agree or disagree with that statement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution notebook\n",
    "[Solution notebook](sklearn_feature_importance_solution.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
